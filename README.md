# Diagnostico de anemias com Keras

![](https://www.foa.unesp.br/Home/ensino/departamentos/cienciasbasicas/histologia/2g-sang-pp1.jpg)


# O projeto
Esse projeto foi desenvolvido em conjunto com a forma√ß√£o da [Alura Deep Learning com Tensorflow e Keras](https://cursos.alura.com.br/formacao-deep-learning-tensorflow-keras), essa experiementa√ß√£o √© uma adapta√ß√£o do projeto que a instrutora fez em aula utilizando a base Iris, como eu gosto de ir fazendo com um tema do interesse em paralelo, acabei esse experimento. A ideia desse texto √© compartilhar um pouco do que aprendi e as possibilidades do Keres/Tensorflow e tamb√©m fortalecer o meu aprendizado. Se voc√™ chegou aqui espero que o texto seja √∫til, e caso voc√™ tenha corre√ß√µes, sugest√µes ou comentarios s√≥ me manda por favor, vou ficar feliz demais! 

# Objetivo
O objetivo deste projeto √© desenvolver um modelo preditivo capaz de identificar diferentes tipos de anemia a partir de exames de sangue utilizando t√©cnicas de aprendizado de m√°quina, mais especificamente redes neurais implementadas com a biblioteca Keras. O sistema busca automatizar o processo de diagn√≥stico, detectando condi√ß√µes como anemia por defici√™ncia de ferro, leucemia, anemias macroc√≠ticas, entre outras, com base em resultados de hemogramas. Por ser parte da forma√ß√£o citada anteriormente n√£o foram exploradas todas as possibilidades e otimiza√ß√µes dispon√≠veis, trabalhando apenas com os aspectos apresentados no curso.

# Descri√ß√£o
O projeto parte de um [conjunto de dados do Kaggle](https://www.kaggle.com/datasets/ehababoelnaga/anemia-types-classification/data) contendo 1281 amostras de exames de sangue, onde cada entrada representa os resultados de exames hematol√≥gicos conforme o exemplo abaixo:

```
    'WBC': 2.740,    # Contagem de gl√≥bulos brancos
    'LYMp': 36.9,   # Porcentagem de linf√≥citos
    'NEUTp': 60.0,  # Porcentagem de neutr√≥filos
    'LYMn': 2.0,    # Contagem absoluta de linf√≥citos 
    'NEUTn': 3.5,   # Contagem absoluta de neutr√≥filos
    'RBC': 4.77,    # Contagem de gl√≥bulos vermelhos 
    'HGB': 13.5,    # Hemoglobina 
    'HCT': 39.7,    # Hemat√≥crito 
    'MCV': 83.2,    # Volume corpuscular m√©dio 
    'MCH': 28.3,    # Hemoglobina corpuscular m√©dia
    'MCHC': 320.0,  # Concentra√ß√£o de hemoglobina corpuscular
    'PLT': 167.0,   # Contagem de plaquetas
    'PDW': 0.15,    # Largura de distribui√ß√£o de plaquetas
    'PCT': 0.02     # Propor√ß√£o de plaquetas
```

Cada entrada est√° associada a um diagn√≥sticos bin√°rio relacionado a diferentes tipos de anemia distribuidos da seguinte forma:

![](graficos_anemias.png)


## Pr√©-Processamento
O dataset do Kaggle possu√≠a uma coluna chamada ‚ÄúDiagnosis‚Äù, contendo os diagn√≥sticos de forma descritiva. Para que o modelo pudesse lidar com essas informa√ß√µes, foi necess√°rio transformar essa coluna em vari√°veis bin√°rias (dummies).

```
diagnosis_dummies = pd.get_dummies(df['Diagnosis'])
df = pd.concat([df, diagnosis_dummies], axis=1)

```


Os dados num√©ricos dos exames de sangue foram normalizados utilizando a t√©cnica de StandardScaler, que padroniza os valores com m√©dia 0 e desvio padr√£o 1. Essa normaliza√ß√£o foi necess√°ria devido √† grande amplitude entre os par√¢metros dos exames de sangue.

![](graficos_parametros.png)
Gr√°fico com os dados n√£o normalizados.

## Modelo

A cria√ß√£o do modelo e suas defini√ß√µes iniciais foram feitas utilizando o m√©todo [`keras.Sequential`](https://keras.io/api/models/sequential/) do Keras, que permite a defini√ß√£o de camadas como  [`Dense`](https://keras.io/api/layers/core_layers/dense/) que s√£o totalmente conectadas. Cada camada cont√©m um n√∫mero definido de neur√¥nios; neste caso, foram utilizadas duas camadas com 64 e 32 neur√¥nios, respectivamente.

```
model = Sequential()

model.add(Input(shape=(X_train.shape[1],)))
model.add(Dense(64, activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(Y_train.shape[1], activation='sigmoid'))

```

Aquilo que chamamos de "neuronios" s√£o denominados perceptrons, eles s√£o a estrutura b√°sica dos modelos de deep learning, considerado o "√°tomo" de uma rede neural. Ele √© composto por uma entrada, um processamento e uma sa√≠da, mimetizando de forma simb√≥lica as fun√ß√µes de um neur√¥nio biol√≥gico (dendritos, ax√¥nio e terminais sin√°pticos). Esse processamento √© expresso pela equa√ß√£o:


$$
y = f(\mathbf{w} \cdot \mathbf{x} + b)
$$


- y = saida
- f = fun√ßa√µ de ativa√ß√£o
- w = pesos
- x = entrada
- b = vies(bias)

Na pr√°tica, com o Keras, a defini√ß√£o da camada Dense corresponde basicamente √† defini√ß√£o dos termos dessa equa√ß√£o, da seguinte forma:

**X (Entradas):** Refere-se √† quantidade de "colunas" ou vari√°veis de entrada que ir√£o alimentar o "neur√¥nio", ou seja, os valores que ser√£o usados para calcular e fazer a infer√™ncia, correspondendo √†s entradas do perceptron. Esse termo √© definido na primeira linha do `keras.Sequential` com `Input(shape=X)`.

**W (Pesos):** Existe um peso para cada entrada que o modelo recebe. Cada peso √© um valor (ou coeficiente) que multiplica a entrada correspondente, conforme descrito na equa√ß√£o. No Keras, os pesos s√£o inicializados pelo par√¢metro [`keras.initializers`](https://keras.io/api/layers/initializers/), onde existem v√°rias op√ß√µes de inicializa√ß√£o. Caso n√£o seja especificado, eles ser√£o iniciados com [`glorot_uniform`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/layers/core.py).

**b (Vi√©s):** O vi√©s √© semelhante aos pesos, mas ele √© somado ao produto das entradas e dos pesos, permitindo mais flexibilidade ao modelo. Isso garante que, mesmo que as entradas sejam zero, a sa√≠da n√£o seja necessariamente zero. No Keras, o vi√©s √© inicializado pelo par√¢metro `bias_initializer`, que por padr√£o √© inicializado com zeros.

**Y (Sa√≠da):** A √∫ltima camada define a sa√≠da, que no nosso caso foi definida com 9 unidades, correspondendo ao n√∫mero de sa√≠das (diagn√≥sticos) presentes no dataset.

Ap√≥s a defini√ß√£o desses par√¢metros, o pr√≥ximo passo √© a fun√ß√£o soma: Ela agrega os valores das entradas multiplicadas pelos pesos, gerando o "valor de ativa√ß√£o", que ser√° passado para a fun√ß√£o de ativa√ß√£o. Esse processo n√£o √© um par√¢metro da camada Dense, mas ocorre automaticamente dentro dela.

**Fun√ß√£o de Ativa√ß√£o:** Ela recebe o valor de ativa√ß√£o e gera a sa√≠da. As camadas ocultas usam a fun√ß√£o de ativa√ß√£o ReLU (Retifica√ß√£o Linear), enquanto a camada de sa√≠da utiliza a fun√ß√£o sigmoid, que √© adequada para problemas de classifica√ß√£o multirr√≥tulo (multilabel).

- ReLu - A fun√ß√£o relu de retifica√ß√£o linear executa uma a√ß√£o bem simples, se o valor √© negativo tem como sa√≠da 0 e se for positivo √© devolvido o pr√≥prio valor. A ReLu √© mais comumente usada por ser computacionalmente eficiente e tamb√©m conseguir lidar bem com o [problema de gradiente desaparecido](https://www.deeplearningbook.com.br/o-problema-da-dissipacao-do-gradiente/) que de forma resumida √© quando os valores os valores dos gradientes, que s√£o necess√°rios para atualizar os pesos durante o aprendizado, se tornam muito pequenos, tornando o processo de aprendizado lento ou ineficaz.

``` activation='relu' ```

- Sigmoid - A fun√ß√£o Sigmoid transforma qualquer valor de entrada em um valor entre 0 e 1, o que a torna ideal para problemas de classifica√ß√£o bin√°ria. Valores grandes se aproximam de 1, e valores pequenos se aproximam de 0, permitindo que a sa√≠da seja interpretada como uma probabilidade.

``` activation='sigmoid' ```

- Softmax - A fun√ß√£o Softmax √© usada em redes neurais para problemas de classifica√ß√£o multiclasse. Ela transforma um conjunto de valores de entrada em probabilidades, distribuindo-as de forma que a soma seja igual a 1. Cada sa√≠da representa a probabilidade de pertencer a uma das classes, sendo a mais alta geralmente a previs√£o do modelo.

``` activation='sigmoid' ```

**O aprendizado aconte√ßa nas fun√ß√µes de somas e ativa√ß√£o**

Ap√≥s a constru√ß√£o e execu√ß√£o do c√≥digo vamos ter criado a estrutura do modelo, o Keras oferece um m√©todo chamado summary() onde √© poss√≠vel visualizar as suas caracter√≠sticas:

```
Model: "sequential"
‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì
‚îÉ Layer (type)                         ‚îÉ Output Shape                ‚îÉ         Param # ‚îÉ
‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©
‚îÇ dense (Dense)                        ‚îÇ (None, 64)                  ‚îÇ             960 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ dense_1 (Dense)                      ‚îÇ (None, 32)                  ‚îÇ           2,080 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ dense_2 (Dense)                      ‚îÇ (None, 9)                   ‚îÇ             297 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
 Total params: 3,337 (13.04 KB)
 Trainable params: 3,337 (13.04 KB)
 Non-trainable params: 0 (0.00 B)

```
Os valores s√£o calculados da seguinte forma

Valor anterior * neur√¥nios + neur√¥nios

A primeira linha com 960 par√¢metros se refere:

14(entrada) * 64(neur√¥nios) + 64 (neur√¥nios) = 960

64 * 32 + 32 = 2080

32 * 9 + 9 = 297

## Treinamento

Para o treinamento, o primeiro passo √© compilar o modelo, atrav√©s do m√©todo [`.compile()`](https://keras.io/api/models/model_training_apis/) onde s√£o ajustados os par√¢metros do modelo.


```
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

```
------------

**Otimizador:**

Otimizadores s√£o algoritmos usados dentro do modelo para efetuar o treinamento. Com base na fun√ß√£o de perda (diferen√ßa entre o resultado correto que o modelo deveria devolver e o predito), os valores de pesos e vieses s√£o ajustados para melhorar a efici√™ncia do modelo.

- Essa abordagem √© chamada de gradiente descendente. Em suma, em cada etapa, este m√©todo verifica, para cada par√¢metro, para que lado a perda do conjunto de treinamento se moveria se voc√™ perturbasse esse par√¢metro apenas um pouco. Em seguida, atualiza o par√¢metro na dire√ß√£o que pode reduzir a perda. [Dive into Deep Learning, 1.2.4. Algoritmos de Otimiza√ß√£o](https://pt.d2l.ai/chapter_introduction/index.html?highlight=fun%C3%A7%C3%A3o%20de%20perda#algoritmos-de-otimizacao)

Existem diversos tipos de otimizadores. No [Keras s√£o disponibilizados 12 tipos](https://keras.io/api/optimizers/). Os algoritmos mais comuns s√£o o Adam, SGD, RMSProp e AdaGrad. De maneira geral:

- Adam √© uma escolha padr√£o que funciona bem em muitos problemas.
- SGD √© √∫til para grandes conjuntos de dados e √© a base de muitos otimizadores modernos.
- RMSProp √© adequado para redes neurais recorrentes (RNNs).
- AdaGrad √© √∫til para dados esparsos. (RUDER, 2016)

Durante o projeto, tentei inicialmente com RMSProp, que foi proposto no curso, mas acabei ficando com o Adam devido aos resultados um pouco melhores. N√£o foi uma grande diferen√ßa, mas os resultados foram melhores.

------------

**Perda:**

A perda ou fun√ß√£o de perda √© um componente do modelo que mede a discrep√¢ncia entre as previs√µes do modelo e os valores esperados. Quando treinamos um modelo, o objetivo principal √© diminuir a fun√ß√£o de perda, de forma que o modelo fa√ßa previs√µes mais precisas. A fun√ß√£o, como entrada, recebe as previs√µes do modelo e os valores reais, retornando um valor escalar que representa o erro. Esse valor √© ent√£o utilizado pelo otimizador para ajustar os pesos da rede neural, em um processo iterativo de tentativa de redu√ß√£o do erro (GOODFELLOW, 2016, p. 82-83).

Existem diversas fun√ß√µes de perda, sendo divididas em 3 grupos: perdas probabil√≠sticas, perdas de regress√£o e Hinge Losses. O Keras disp√µe de uma lista de fun√ß√µes que podem ser implementadas na compila√ß√£o, que podem ser [consultadas na documenta√ß√£o](https://keras.io/api/losses/). Neste caso, foi utilizada a "binary crossentropy" pois foi realizado o processo de get_dummies, logo o modelo faz uma classifica√ß√£o bin√°ria dizendo se as entradas pertencem ou n√£o a determinada categoria.

------------

**Metricas:** 

A defini√ß√£o da m√©trica √© um fator fundamental em um modelo, pois √© a partir dela que, futuramente, ser√£o realizadas otimiza√ß√µes, sempre usando os seus resultados como guia. Dessa forma, determinar os objetivos em termos de qual m√©trica utilizar √© sempre um primeiro passo necess√°rio (GOODFELLOW, 2016, p. 422).

Neste projeto, foi escolhida a m√©trica de acur√°cia para entender quantas vezes um determinado resultado de exame foi diagnosticado corretamente. [Op√ß√µes de metricas do Keras](https://keras.io/api/metrics/)

$$
{Acur√°cia} = \frac{\text{ N√∫mero de diagn√≥sticos corretos }}{\text{ Total de diagn√≥sticos }}
$$

### Fit Model

Ap√≥s a compila√ß√£o do modelo √© executado de fato o treinamento:

    epocas=50
    historico = model.fit(X_train, Y_train, epochs=epocas, batch_size=32, validation_split=0.2)

**√âpocas e batch size:**

A defini√ß√£o das √©pocas no treinamento significa a quantidade de vezes que o modelo vai percorrer todos os dados para realizar o treinamento e atualizar os pesos e vises. Por√©m n√£o v√£o ser percorridos todos os dados de uma vez s√≥, esses dados v√£o ser processados em lotes, os batchs. No nosso caso foram definidos inicialmente 50 √©pocas com um batch_size de 32 isso quer dizer que com um total de 1280 amostras vamos ter o seguinte comportamento: 

- a cada √©poca, o modelo processar√° 1280 √∑ 32 = 40 batches.
-  O modelo ajusta seus pesos 40 vezes por √©poca (ap√≥s cada batch de 32 amostras).
-  Ap√≥s 50 √©pocas, o modelo ter√° ajustado os pesos 50 √ó 40 = 2000 vezes. 

**Validation_Split:** Par√¢metro que defini a quantidade de dados que v√£o ser utilizados no processo de valida√ß√£o do modelo. 

## Valida√ß√£o


Aqui est√° a corre√ß√£o ortogr√°fica:

Ap√≥s o treino do modelo, partimos para o processo de valida√ß√£o, verificando a perda e a acur√°cia do modelo em rela√ß√£o aos dados de teste e valida√ß√£o que definimos anteriormente. Podemos observar que, ao executar o treino do modelo, salvamos sua sa√≠da em uma vari√°vel `historico`,o que possibilita a utiliza√ß√£o do m√©todo [`history`](https://keras.io/api/models/model_training_apis/#:~:text=Returns,values%20(if%20applicable).) sse m√©todo retorna os valores de perda e os valores de m√©trica do modelo (no nosso caso, "accuracy"). Ele retorna um dicion√°rio, o que permite que plotemos os resultados em gr√°ficos, facilitando assim a visualiza√ß√£o dos resultados do modelo.

- Gr√°ficos concatenados usando o seaborn direto no history

![](his_graf1.png)




- Gr√°ficos separados usando a uma fun√ß√£o para construir o plot.

![](his_graf2.png)


Al√©m dos gr√°ficos tamb√©m √© poss√≠vel testar o modelo e visualizar uma analise resumida pelo m√©todo `evaluate` que apresenta o seguinte retorno:

```
9/9 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 2ms/step - accuracy: 0.8223 - loss: 0.0934 Loss: 0.08661733567714691, Accuracy: 0.844357967376709 
```

Utilizando os resultados da perda e acur√°cia no treino e valida√ß√£o podemos ter uma orienta√ß√£o de quais ajustes que fizemos impactam o modelo. Se voc√™ [olhar no notebook](Keras_diagnostico_de_anemias.ipynb) do projeto vai ver que tentei fazer algumas mudan√ßas no ‚Äúchute‚Äù mesmo para testar diferentes valores dos par√¢metros. Existem t√©cnicas mais estruturadas para busca de parametros por tentativa e erro como o Random Search e Grid Search e que no Keras fazem parte do [KerasTuner](https://keras.io/api/keras_tuner/tuners/) que ajuda na otimiza√ß√£o dos modelos buscando os melhores hiperparametros.

# Testando o modelo

Para testar o modelo eu separei alguns dados do pr√≥prio dataset de forma manual, extraindo esses dados antes de dar in√≠cio ao projeto. A exibi√ß√£o dos resultados de hemograma pode ser um pouco diferente, n√£o os resultados, mas somente sua exibi√ß√£o. Dependendo do sistema de interface, da metodologia no laborat√≥rio e do equipamento no qual eles foram feitos pode ser que seja realizado com ou sem diferencial, e j√° tenha os valores calculados. Os resultados utilizados para teste s√£o de um hemograma sem diferencial. 

```
# exame = {
#     'WBC': 2.740,   # Contagem de gl√≥bulos brancos
#     'LYMp': 36.9,   # Porcentagem de linf√≥citos
#     'NEUTp': 60.0,  # Porcentagem de neutr√≥filos
#     'LYMn': 2.0,    # Contagem absoluta de linf√≥citos
#     'NEUTn': 3.5,   # Contagem absoluta de neutr√≥filos
#     'RBC': 4.77,    # Contagem de gl√≥bulos vermelhos
#     'HGB': 13.5,    # Hemoglobina
#     'HCT': 39.7,    # Hemat√≥crito
#     'MCV': 83.2,    # Volume corpuscular m√©dio
#     'MCH': 28.3,    # Hemoglobina corpuscular m√©dia
#     'MCHC': 320.0,  # Concentra√ß√£o de hemoglobina corpuscular
#     'PLT': 167.0,   # Contagem de plaquetas
#     'PDW': 0.15,    # Largura de distribui√ß√£o de plaquetas
#     'PCT': 0.02     # Propor√ß√£o de plaquetas
# }

exame1 = {
    'WBC': 8.67,
    'LYMp': 25.8,
    'NEUTp': 77.5,
    'LYMn': 1.88,
    'NEUTn': 5.14,
    'RBC': 3.58,
    'HGB': 10.2,
    'HCT': 46.1,
    'MCV': 102.8,
    'MCH': 28.5,
    'MCHC': 27.7,
    'PLT': 272,
    'PDW': 14.31,
    'PCT': 0.26
}

amostra_exame = pd.DataFrame([exame])
amostra_exame_normalizada = scaler.transform(amostra_exame)
```

Esses resultados estavam identificados no dataset como saud√°vel(exame) e  anemia normoc√≠tica e normocr√¥mica(exame1). Como esse √© um projeto de aprendizagem e experimenta√ß√£o da biblioteca eu n√£o fui t√£o a fundo na valida√ß√£o dos resultados e valores, apenas utilizei algumas indica√ß√µes do Manual de interpreta√ß√£o do Failace para manter o m√≠nimo de coer√™ncia. 

Na c√©lula junto com o dicion√°rio dos resultados tamb√©m est√° a transforma√ß√£o para um dataframe  e a normaliza√ß√£o dos valores para ficar de acordo com o formato que foi utilizado para o treinamento do modelo. Ap√≥s realizar essas transforma√ß√µes utilizamos o m√©todo [`predict()`](https://keras.io/api/models/model_training_apis/#:~:text=Generates%20output%20predictions%20for%20the%20input%20samples) para gerar a sa√≠da do modelo.
Na linha abaixo observamos que h√° uma transforma√ß√£o na vari√°vel que salva a sa√≠da de model.predict:

    predictions = (predictions > 0.5).astype(int)

Como utilizamos as fun√ß√µes de sa√≠da sigmoid ou softmax(ver testes) ambas retornam uma probabilidade de os valores de entrada pertencer a uma determinada classe. Nessa situa√ß√£o em espec√≠fico est√° fazendo um arredondamento para 0 ou 1 dependendo da probabilidade predita. A ideia com essa transforma√ß√£o √© criar um array com valores bin√°rios referente a cada um dos diagn√≥sticos, de forma que fique mais f√°cil a visualiza√ß√£o da sa√≠da.  Em uma situa√ß√£o real seria necess√°rio um modelo com maior acur√°cia para aumentar esse valor  de arrendondamento para garantia a precis√£o das previs√µes. 

Ap√≥s isso fizemos uma lista com os diagn√≥sticos presentes no dataset:
```
# Nomes das colunas de diagn√≥stico
diagnosis_columns = [
    'Diagnosis_Healthy', 'Diagnosis_Iron deficiency anemia',
    'Diagnosis_Leukemia', 'Diagnosis_Leukemia with thrombocytopenia',
    'Diagnosis_Macrocytic anemia', 'Diagnosis_Normocytic hypochromic anemia',
    'Diagnosis_Normocytic normochromic anemia', 'Diagnosis_Other microcytic anemia',
    'Diagnosis_Thrombocytopenia'
]
```
Utilizamos a fun√ß√£o zip para combinar a lista dos resultados e as predi√ß√µes do modelo e com um la√ßo simples podemos exibir os resultados de sa√≠da

```
# Exibir as previs√µes
for col, pred in zip(diagnosis_columns, predictions[0]):
    print(f"{col}: {'Positivo' if pred == 1 else 'Negativo'}")
```

Tendo com sa√≠da para os dicionarios:

Exame:

    Diagnosis_Healthy: Positivo
    Diagnosis_Iron deficiency anemia: Negativo
    Diagnosis_Leukemia: Negativo
    Diagnosis_Leukemia with thrombocytopenia: Negativo
    Diagnosis_Macrocytic anemia: Negativo
    Diagnosis_Normocytic hypochromic anemia: Negativo
    Diagnosis_Normocytic normochromic anemia: Negativo
    Diagnosis_Other microcytic anemia: Negativo
    Diagnosis_Thrombocytopenia: Negativo

Exame1:

    Diagnosis_Healthy: Negativo
    Diagnosis_Iron deficiency anemia: Negativo
    Diagnosis_Leukemia: Negativo
    Diagnosis_Leukemia with thrombocytopenia: Negativo
    Diagnosis_Macrocytic anemia: Negativo
    Diagnosis_Normocytic hypochromic anemia: Positivo
    Diagnosis_Normocytic normochromic anemia: Negativo
    Diagnosis_Other microcytic anemia: Negativo
    Diagnosis_Thrombocytopenia: Negativo

Se voc√™ chegou at√© aqui, parab√©ns! E obrigado por ter dedicado esse tempo para acompanhar meu processo de aprendizagem e minhas experimenta√ß√µes com o Keras. Novamente, se voc√™ tiver dicas, observa√ß√µes ou apenas queira conversar sobre s√≥ me chamar, vou ficar feliz de poder trocar com pessoas que se interessam pelo assunto. üôèüèΩ

# Referencias

GOODFELLOW, Ian; BENGIO, Yoshua; COURVILLE, Aaron. Deep Learning. MIT Press, 2016.

RUDER, Sebastian. An Overview of Gradient Descent Optimization Algorithms. arXiv, 2016. Dispon√≠vel em: https://arxiv.org/abs/1609.04747. Acesso em: 14 out. 2024

Dive into Deep Learning - Interactive deep learning book with code, math, and discussions
https://pt.d2l.ai/index.html

Deep Learning Book Brasil
https://www.deeplearningbook.com.br/

‚ÄåFAILACE, R. Hemograma: manual de interpreta√ß√£o. [s.l.] Artmed Editora, 2015.
